{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          DataCollatorWithPadding, Trainer, TrainingArguments)\n",
    "import numpy as np\n",
    "\n",
    "from data import SharedTaskData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function map_label at 0x7f323f8bcc10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 17196.44ex/s]\n",
      "Casting to class labels: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 131.16ba/s]\n",
      "Casting the dataset: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 187.47ba/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 750/750 [00:00<00:00, 19637.97ex/s]\n",
      "Casting to class labels: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 157.67ba/s]\n",
      "Casting the dataset: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 206.09ba/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 202/202 [00:00<00:00, 20020.07ex/s]\n",
      "Casting the dataset: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 309.70ba/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 202/202 [00:00<00:00, 20013.92ex/s]\n",
      "Casting the dataset: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 431.56ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = SharedTaskData(\"TaskA_train.csv\")\n",
    "dev_data = SharedTaskData(\"TaskA_dev.csv\")\n",
    "\n",
    "train_dataset_novelty = train_data.convert_to_hf_dataset(\"novelty\")\n",
    "train_dataset_validity = train_data.convert_to_hf_dataset(\"validity\")\n",
    "# Use feature mapping from training dataset to ensure features are mapped correctly\n",
    "dev_dataset_novelty = dev_data.convert_to_hf_dataset(\"novelty\", features=train_dataset_novelty.features)\n",
    "dev_dataset_validity = dev_data.convert_to_hf_dataset(\"validity\", features=train_dataset_validity.features)\n",
    "\n",
    "# Make sure internal label mapping is identical across datasets\n",
    "assert train_dataset_validity.features['validity_str']._str2int == dev_dataset_validity.features['validity_str']._str2int\n",
    "assert train_dataset_novelty.features['novelty_str']._str2int == dev_dataset_novelty.features['novelty_str']._str2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deafisible': 0, 'not-valid': 1, 'valid': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f33186c9e20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# investigate validity labels\n",
    "print(train_dataset_validity.features['validity_str']._str2int)\n",
    "sns.displot(train_dataset_validity['validity_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'borderline novel': 0, 'not-novel': 1, 'novel': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f323f51ef70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# .. and novelty labels\n",
    "print(train_dataset_novelty.features['novelty_str']._str2int)\n",
    "sns.displot(train_dataset_novelty['novelty_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.50      0.02         2\n",
      "           1       0.56      0.47      0.51       118\n",
      "           2       0.00      0.00      0.00        82\n",
      "\n",
      "    accuracy                           0.28       202\n",
      "   macro avg       0.19      0.32      0.18       202\n",
      "weighted avg       0.32      0.28      0.30       202\n",
      "\n",
      "Validity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.67      0.04         3\n",
      "           1       0.37      0.53      0.44        74\n",
      "           2       0.00      0.00      0.00       125\n",
      "\n",
      "    accuracy                           0.20       202\n",
      "   macro avg       0.13      0.40      0.16       202\n",
      "weighted avg       0.14      0.20      0.16       202\n",
      "\n",
      "Novelty\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.58      1.00      0.74       118\n",
      "           2       0.00      0.00      0.00        82\n",
      "\n",
      "    accuracy                           0.58       202\n",
      "   macro avg       0.19      0.33      0.25       202\n",
      "weighted avg       0.34      0.58      0.43       202\n",
      "\n",
      "Validity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00        74\n",
      "           2       0.62      1.00      0.76       125\n",
      "\n",
      "    accuracy                           0.62       202\n",
      "   macro avg       0.21      0.33      0.25       202\n",
      "weighted avg       0.38      0.62      0.47       202\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Random Baseline novelty\n",
    "print(\"Novelty\")\n",
    "y_true = np.array(dev_dataset_novelty['novelty_str'])\n",
    "y_pred = np.random.randint(low=0, high=2, size=len(dev_dataset_novelty))\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Random Baseline validity\n",
    "print(\"Validity\")\n",
    "y_true = np.array(dev_dataset_validity['validity_str'])\n",
    "y_pred = np.random.randint(low=0, high=2, size=len(dev_dataset_validity))\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Majority Baseline novelty\n",
    "print(\"Novelty\")\n",
    "y_true = np.array(dev_dataset_novelty['novelty_str'])\n",
    "y_pred = np.array([train_dataset_novelty.features['novelty_str']._str2int['not-novel']] * len(dev_dataset_novelty))\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Majority Baseline validity\n",
    "print(\"Validity\")\n",
    "y_true = np.array(dev_dataset_validity['validity_str'])\n",
    "y_pred = np.array([train_dataset_validity.features['validity_str']._str2int['valid']] * len(dev_dataset_validity))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data imbalance\n",
    "We suffer from some data imbalance so we may need to oversample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.81ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 46.76ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 21.12ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 44.71ba/s]\n"
     ]
    }
   ],
   "source": [
    "# We focus on predicting Validity label for now\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=train_dataset_validity.features['validity_str'].num_classes,\n",
    "    label2id=train_dataset_validity.features['validity_str']._str2int, \n",
    "    id2label={v:k for k, v in train_dataset_validity.features['validity_str']._str2int.items()},\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize_function_val(examples):\n",
    "    batch_size = len(examples['Premise'])\n",
    "    batched_inputs = [\n",
    "        examples['topic'][i] + tokenizer.sep_token + \\\n",
    "        examples['Premise'][i] + tokenizer.sep_token + \\\n",
    "        examples['Conclusion'][i] for i in range(batch_size)\n",
    "    ]\n",
    "    samples = tokenizer(batched_inputs, truncation=True, padding=True)\n",
    "    samples['labels'] = examples['validity_str']\n",
    "    return samples \n",
    "\n",
    "def tokenize_function_nov(examples):\n",
    "    batch_size = len(examples['Premise'])\n",
    "    batched_inputs = [\n",
    "        examples['topic'][i] + tokenizer.sep_token + \\\n",
    "        examples['Premise'][i] + tokenizer.sep_token + \\\n",
    "        examples['Conclusion'][i] for i in range(batch_size)\n",
    "    ]\n",
    "    samples = tokenizer(batched_inputs, truncation=True, padding=True)\n",
    "    samples['labels'] = examples['novelty_str']\n",
    "    return samples \n",
    "\n",
    "\n",
    "tokenized_train_dataset_validity = train_dataset_validity.map(tokenize_function_val, batched=True)\n",
    "tokenized_dev_dataset_validity = dev_dataset_validity.map(tokenize_function_val, batched=True)\n",
    "tokenized_train_dataset_validity.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "tokenized_dev_dataset_validity.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "tokenized_train_dataset_novelty = train_dataset_novelty.map(tokenize_function_nov, batched=True)\n",
    "tokenized_dev_dataset_novelty = dev_dataset_novelty.map(tokenize_function_nov, batched=True)\n",
    "tokenized_train_dataset_novelty.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "tokenized_dev_dataset_novelty.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "\n",
    "def single_label_metrics(predictions, labels):\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    preds = torch.Tensor(predictions)\n",
    "    probs = softmax(preds)\n",
    "    y_pred = torch.argmax(probs, dim=1)\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return {'f1': f1_micro_average, 'accuracy': accuracy}\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    return single_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"argmining2022_trainer\",\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_dataset_validity,\n",
    "    eval_dataset=tokenized_dev_dataset_validity,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task Learning\n",
    "Code inspired by [here](https://colab.research.google.com/github/zphang/zphang.github.io/blob/master/files/notebooks/Multi_task_Training_with_Transformers_NLP.ipynb#scrollTo=yrtS3ZeSsoZw).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MultitaskModel\n",
    "from trainers import MultitaskTrainer, NLPDataCollator\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139850758737984\n",
      "139850758737984\n",
      "139850758737984\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "multitask_model = MultitaskModel.create(\n",
    "    model_name=model_name,\n",
    "    model_type_dict={\n",
    "        \"novelty\": transformers.AutoModelForSequenceClassification,\n",
    "        \"validity\": transformers.AutoModelForSequenceClassification,\n",
    "    },\n",
    "    model_config_dict={\n",
    "        \"novelty\": transformers.AutoConfig.from_pretrained(model_name, num_labels=2),\n",
    "        \"validity\": transformers.AutoConfig.from_pretrained(model_name, num_labels=2),\n",
    "    },\n",
    ")\n",
    "\n",
    "if model_name.startswith(\"roberta-\"):\n",
    "    print(multitask_model.encoder.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"novelty\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "    print(multitask_model.taskmodels_dict[\"validity\"].roberta.embeddings.word_embeddings.weight.data_ptr())\n",
    "else:\n",
    "    print(\"Exercise for the reader: add a check for other model architectures =)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3050 Ti Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = {\n",
    "    \"validity\": tokenized_train_dataset_validity,\n",
    "    \"novelty\": tokenized_train_dataset_novelty\n",
    "}\n",
    "trainer = MultitaskTrainer(\n",
    "    model=multitask_model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./models/multitask_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=1e-5,\n",
    "        do_train=True,\n",
    "        num_train_epochs=3,\n",
    "        # Adjust batch size if this doesn't fit on the Colab GPU\n",
    "        per_device_train_batch_size=8,  \n",
    "        save_steps=3000,\n",
    "    ),\n",
    "    data_collator=NLPDataCollator(),\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m0re/projects/phd/venvs/vargmining/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/m0re/projects/phd/argmining2022/train_arg.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/m0re/projects/phd/argmining2022/train_arg.ipynb#ch0000017?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/projects/phd/venvs/vargmining/lib/python3.9/site-packages/transformers/trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1406\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1408\u001b[0m )\n\u001b[0;32m-> 1409\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1410\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1411\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1412\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1413\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1414\u001b[0m )\n",
      "File \u001b[0;32m~/projects/phd/venvs/vargmining/lib/python3.9/site-packages/transformers/trainer.py:1651\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1649\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1650\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1651\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1653\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1654\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1655\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1656\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1657\u001b[0m ):\n\u001b[1;32m   1658\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/projects/phd/venvs/vargmining/lib/python3.9/site-packages/transformers/trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2342\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2344\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2345\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2348\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/phd/venvs/vargmining/lib/python3.9/site-packages/transformers/trainer.py:2377\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2376\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2377\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2378\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2379\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/projects/phd/venvs/vargmining/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/phd/argmining2022/models.py:55\u001b[0m, in \u001b[0;36mMultitaskModel.forward\u001b[0;34m(self, task_name, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, task_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtaskmodels_dict[task_name](\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/phd/venvs/vargmining/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/phd/venvs/vargmining/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1206\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1206\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1207\u001b[0m     input_ids,\n\u001b[1;32m   1208\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1209\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1210\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1211\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1212\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1213\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1214\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1215\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1216\u001b[0m )\n\u001b[1;32m   1217\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1218\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/projects/phd/venvs/vargmining/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/phd/venvs/vargmining/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:821\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    817\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    819\u001b[0m \u001b[39m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[39m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_extended_attention_mask(attention_mask, input_shape)\n\u001b[1;32m    823\u001b[0m \u001b[39m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[39m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_decoder \u001b[39mand\u001b[39;00m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/phd/venvs/vargmining/lib/python3.9/site-packages/transformers/modeling_utils.py:839\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    831\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrong shape for input_ids (shape \u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m) or attention_mask (shape \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    834\u001b[0m \u001b[39m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[39m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[39m# positions we want to attend and -10000.0 for masked positions.\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[39m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[39m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m extended_attention_mask\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype)  \u001b[39m# fp16 compatibility\u001b[39;00m\n\u001b[1;32m    840\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m extended_attention_mask) \u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m10000.0\u001b[39m\n\u001b[1;32m    841\u001b[0m \u001b[39mreturn\u001b[39;00m extended_attention_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
