{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc1ec4e6-fcf4-4bd6-835f-65172eb9053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0551cdf5-ced6-4123-a168-067bb0dcca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e7055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = '/Users/myrthereuver/Documents/GitHub/argmining2022/TaskA_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ef43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dev = '/Users/myrthereuver/Documents/GitHub/argmining2022/TaskA_dev.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e17909",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_test = '/Users/myrthereuver/Documents/GitHub/argmining2022/TaskA_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86888f-2bb6-4d3c-abf0-d2a05cb20f46",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4304b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(input_file):\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    \n",
    "    df_current = pd.read_csv(input_file)\n",
    "    \n",
    "    # changing neutral to 1, as specified in the paper and data\n",
    "    df_current.loc[df_current[\"Validity\"] == 0, \"Validity\"] = 1\n",
    "    df_current.loc[df_current[\"Novelty\"] == 0, \"Novelty\"] = 1\n",
    "    \n",
    "    # adding the premise and conclusion to one text\n",
    "    df_current[\"examples\"] = df_current[\"Premise\"]+\" AND \"+df_current[\"Conclusion\"]\n",
    "    df_current[\"examples_topic\"] = df_current[\"topic\"]+ \" AND \"+df_current[\"Premise\"]+\" AND \"+df_current[\"Conclusion\"]\n",
    "    \n",
    "    # stemming\n",
    "    df_current[\"stemmed\"] = df_current[\"examples\"].apply(lambda row: \" \".join([stemmer.stem(w) for w in row.split(\" \")]))\n",
    "    df_current[\"stemmed_topic\"] = df_current[\"examples_topic\"].apply(lambda row: \" \".join([stemmer.stem(w) for w in row.split(\" \")]))\n",
    "    return df_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1962f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = create_df(file_train)\n",
    "data_dev = create_df(file_dev)\n",
    "data_test = create_df(file_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07db1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    595\n",
       " 1    155\n",
       "Name: Novelty, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"Novelty\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce42b453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bc1e317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trying 9/11 terror suspects in NYC courts             90\n",
       "Trying terrorist suspects in civilian courts          85\n",
       "US health care reform                                 80\n",
       "US offshore oil drilling                              70\n",
       "US and NATO intervention in Libya                     45\n",
       "Torture                                               45\n",
       "US-Indian nuclear deal                                35\n",
       "Two-state solution to Israeli-Palestinian conflict    35\n",
       "United Nations Standing Army                          30\n",
       "Turkey EU membership                                  30\n",
       "Unilateral US military strike inside Pakistan         25\n",
       "UN Security Council veto                              25\n",
       "Using sanctions to end child labor                    20\n",
       "US debt ceiling deal                                  20\n",
       "Twin Towers reconstruction                            20\n",
       "US electoral college                                  15\n",
       "Two-party system                                      15\n",
       "Underground nuclear waste storage                     15\n",
       "United Nations No Growth Budgets                      15\n",
       "TV viewing is harmful to children                     15\n",
       "U.S. Withdrawal From the United Nations               10\n",
       "Trade vs aid                                          10\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ed5216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "print(len(data_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3e50a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    }
   ],
   "source": [
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf218a-b907-4beb-a52e-fd8371137e74",
   "metadata": {},
   "source": [
    "### SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e0dce-7a09-4a36-b33e-19ad4d93232a",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f9aade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_hyperparam = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', svm.LinearSVC(C=0.09)),\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e7eb3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_model(model, validation, label, version_text):\n",
    "    from sklearn import metrics\n",
    "\n",
    "    predicted = model.predict(validation[version_text])\n",
    "  \n",
    "    print(metrics.classification_report(validation[label], predicted, digits=4))\n",
    "    accuracy = np.mean(predicted == validation[label])\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0683b42",
   "metadata": {},
   "source": [
    "### Validity, Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a86ca470",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val = text_clf_hyperparam.fit(data_train.stemmed, data_train.Validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8779195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.6053    0.3108    0.4107        74\n",
      "           1     0.6890    0.8828    0.7740       128\n",
      "\n",
      "    accuracy                         0.6733       202\n",
      "   macro avg     0.6471    0.5968    0.5923       202\n",
      "weighted avg     0.6583    0.6733    0.6409       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_val = results_model(model_val, data_dev, \"Validity\", \"stemmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2282289b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.4211    0.4660    0.4424       206\n",
      "           1     0.6233    0.5796    0.6007       314\n",
      "\n",
      "    accuracy                         0.5346       520\n",
      "   macro avg     0.5222    0.5228    0.5215       520\n",
      "weighted avg     0.5432    0.5346    0.5380       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_val_test = results_model(model_val, data_test, \"Validity\", \"stemmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a8316",
   "metadata": {},
   "source": [
    "### Validity, Topic+stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5497093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val_top = text_clf_hyperparam.fit(data_train.stemmed_topic, data_train.Validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22868424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.5263    0.2703    0.3571        74\n",
      "           1     0.6707    0.8594    0.7534       128\n",
      "\n",
      "    accuracy                         0.6436       202\n",
      "   macro avg     0.5985    0.5648    0.5553       202\n",
      "weighted avg     0.6178    0.6436    0.6083       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_val_top = results_model(model_val_top, data_dev, \"Validity\", \"stemmed_topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac3904dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.4382    0.5340    0.4814       206\n",
      "           1     0.6431    0.5510    0.5935       314\n",
      "\n",
      "    accuracy                         0.5442       520\n",
      "   macro avg     0.5407    0.5425    0.5374       520\n",
      "weighted avg     0.5620    0.5442    0.5491       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_val_topic_test = results_model(model_val_top, data_test, \"Validity\", \"stemmed_topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a41d4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_hyperparam_nov = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', svm.LinearSVC(C=4.7)),\n",
    "     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b09ad",
   "metadata": {},
   "source": [
    "### Novelty, Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3fe0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nov = text_clf_hyperparam_nov.fit(data_train.stemmed, data_train.Novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f9ecc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.5787    0.9661    0.7238       118\n",
      "           1     0.2000    0.0119    0.0225        84\n",
      "\n",
      "    accuracy                         0.5693       202\n",
      "   macro avg     0.3893    0.4890    0.3731       202\n",
      "weighted avg     0.4212    0.5693    0.4322       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_nov = results_model(model_nov, data_dev, \"Novelty\", \"stemmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9672eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.5714    0.9796    0.7218       294\n",
      "           1     0.6250    0.0442    0.0826       226\n",
      "\n",
      "    accuracy                         0.5731       520\n",
      "   macro avg     0.5982    0.5119    0.4022       520\n",
      "weighted avg     0.5947    0.5731    0.4440       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_nov_test = results_model(model_nov, data_test, \"Novelty\", \"stemmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f30867",
   "metadata": {},
   "source": [
    "### Novelty, Topic+stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cff4b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nov_top = text_clf_hyperparam_nov.fit(data_train.stemmed_topic, data_train.Novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "351da7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.5821    0.9915    0.7335       118\n",
      "           1     0.0000    0.0000    0.0000        84\n",
      "\n",
      "    accuracy                         0.5792       202\n",
      "   macro avg     0.2910    0.4958    0.3668       202\n",
      "weighted avg     0.3400    0.5792    0.4285       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_nov_top = results_model(model_nov_top, data_dev, \"Novelty\", \"stemmed_topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dca83f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.5673    0.9898    0.7212       294\n",
      "           1     0.5714    0.0177    0.0343       226\n",
      "\n",
      "    accuracy                         0.5673       520\n",
      "   macro avg     0.5693    0.5037    0.3778       520\n",
      "weighted avg     0.5691    0.5673    0.4227       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_nov_top_test = results_model(model_nov_top, data_test, \"Novelty\", \"stemmed_topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e06cd96",
   "metadata": {},
   "source": [
    "### eval on Shared Task organizer's metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e110f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy\n",
    "\n",
    "def val_nov_metric(is_validity: numpy.ndarray, should_validity: numpy.ndarray, is_novelty: numpy.ndarray, should_novelty: numpy.ndarray) -> Dict[str, float]:\n",
    "    ret = dict()\n",
    "\n",
    "    ret_base_help = {\n",
    "        \"true_positive_validity\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_validity >= .5, should_validity >= .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"true_negative_validity\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_validity < .5, should_validity < .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"true_positive_novelty\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_novelty >= .5, should_novelty >= .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"true_negative_novelty\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_novelty < .5, should_novelty < .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"true_positive_valid_novel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_validity >= .5, is_novelty >= .5,\n",
    "                                   should_validity >= .5, should_novelty >= .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"true_positive_nonvalid_novel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_validity < .5, is_novelty >= .5,\n",
    "                                   should_validity < .5, should_novelty >= .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"true_positive_valid_nonnovel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_validity >= .5, is_novelty < .5,\n",
    "                                   should_validity >= .5, should_novelty < .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"true_positive_nonvalid_nonnovel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_validity < .5, is_novelty < .5,\n",
    "                                   should_validity < .5, should_novelty < .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"classified_positive_validity\": numpy.sum(numpy.where(is_validity >= .5, 1, 0)),\n",
    "        \"classified_negative_validity\": numpy.sum(numpy.where(is_validity < .5, 1, 0)),\n",
    "        \"classified_positive_novelty\": numpy.sum(numpy.where(is_novelty >= .5, 1, 0)),\n",
    "        \"classified_negative_novelty\": numpy.sum(numpy.where(is_novelty < .5, 1, 0)),\n",
    "        \"classified_positive_valid_novel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_validity >= .5, is_novelty >= .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"classified_positive_nonvalid_novel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_validity < .5, is_novelty >= .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"classified_positive_valid_nonnovel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_validity >= .5, is_novelty < .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"classified_positive_nonvalid_nonnovel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([is_validity < .5, is_novelty < .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"indeed_positive_validity\": numpy.sum(numpy.where(should_validity >= .5, 1, 0)),\n",
    "        \"indeed_negative_validity\": numpy.sum(numpy.where(should_validity < .5, 1, 0)),\n",
    "        \"indeed_positive_novelty\": numpy.sum(numpy.where(should_novelty >= .5, 1, 0)),\n",
    "        \"indeed_negative_novelty\": numpy.sum(numpy.where(should_novelty < .5, 1, 0)),\n",
    "        \"indeed_positive_valid_novel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([should_validity >= .5, should_novelty >= .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"indeed_positive_nonvalid_novel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([should_validity < .5, should_novelty >= .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"indeed_positive_valid_nonnovel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([should_validity >= .5, should_novelty < .5]), axis=0),\n",
    "            1, 0)),\n",
    "        \"indeed_positive_nonvalid_nonnovel\": numpy.sum(numpy.where(\n",
    "            numpy.all(numpy.stack([should_validity < .5, should_novelty < .5]), axis=0),\n",
    "            1, 0)),\n",
    "    }\n",
    "\n",
    "    ret_help = {\n",
    "        \"precision_validity\": ret_base_help[\"true_positive_validity\"] /\n",
    "                              max(1, ret_base_help[\"classified_positive_validity\"]),\n",
    "        \"precision_novelty\": ret_base_help[\"true_positive_novelty\"] /\n",
    "                             max(1, ret_base_help[\"classified_positive_novelty\"]),\n",
    "        \"recall_validity\": ret_base_help[\"true_positive_validity\"] /\n",
    "                           max(1, ret_base_help[\"indeed_positive_validity\"]),\n",
    "        \"recall_novelty\": ret_base_help[\"true_positive_novelty\"] /\n",
    "                          max(1, ret_base_help[\"indeed_positive_novelty\"]),\n",
    "        \"precision_val_neg\": ret_base_help[\"true_negative_validity\"] /\n",
    "                              max(1, ret_base_help[\"classified_negative_validity\"]),\n",
    "        \"precision_nov_neg\": ret_base_help[\"true_negative_novelty\"] /\n",
    "                             max(1, ret_base_help[\"classified_negative_novelty\"]),\n",
    "        \"recall_val_neg\": ret_base_help[\"true_negative_validity\"] /\n",
    "                           max(1, ret_base_help[\"indeed_negative_validity\"]),\n",
    "        \"recall_nov_neg\": ret_base_help[\"true_negative_novelty\"] /\n",
    "                          max(1, ret_base_help[\"indeed_negative_novelty\"]),\n",
    "        \"precision_valid_novel\": ret_base_help[\"true_positive_valid_novel\"] /\n",
    "                                 max(1, ret_base_help[\"classified_positive_valid_novel\"]),\n",
    "        \"precision_valid_nonnovel\": ret_base_help[\"true_positive_valid_nonnovel\"] /\n",
    "                                    max(1, ret_base_help[\"classified_positive_valid_nonnovel\"]),\n",
    "        \"precision_nonvalid_novel\": ret_base_help[\"true_positive_nonvalid_novel\"] /\n",
    "                                    max(1, ret_base_help[\"classified_positive_nonvalid_novel\"]),\n",
    "        \"precision_nonvalid_nonnovel\": ret_base_help[\"true_positive_nonvalid_nonnovel\"] /\n",
    "                                       max(1, ret_base_help[\"classified_positive_nonvalid_nonnovel\"]),\n",
    "        \"recall_valid_novel\": ret_base_help[\"true_positive_valid_novel\"] /\n",
    "                              max(1, ret_base_help[\"indeed_positive_valid_novel\"]),\n",
    "        \"recall_valid_nonnovel\": ret_base_help[\"true_positive_valid_nonnovel\"] /\n",
    "                                 max(1, ret_base_help[\"indeed_positive_valid_nonnovel\"]),\n",
    "        \"recall_nonvalid_novel\": ret_base_help[\"true_positive_nonvalid_novel\"] /\n",
    "                                 max(1, ret_base_help[\"indeed_positive_nonvalid_novel\"]),\n",
    "        \"recall_nonvalid_nonnovel\": ret_base_help[\"true_positive_nonvalid_nonnovel\"] /\n",
    "                                    max(1, ret_base_help[\"indeed_positive_nonvalid_nonnovel\"])\n",
    "    }\n",
    "\n",
    "    ret.update({\n",
    "        \"f1_validity\": 2 * ret_help[\"precision_validity\"] * ret_help[\"recall_validity\"] / max(1e-4, ret_help[\"precision_validity\"] + ret_help[\"recall_validity\"]),\n",
    "        \"f1_novelty\": 2 * ret_help[\"precision_novelty\"] * ret_help[\"recall_novelty\"] / max(1e-4, ret_help[\"precision_novelty\"] + ret_help[\"recall_novelty\"]),\n",
    "        \"f1_val_neg\": 2 * ret_help[\"precision_val_neg\"] * ret_help[\"recall_val_neg\"] / max(1e-4, ret_help[\"precision_val_neg\"] + ret_help[\"recall_val_neg\"]),\n",
    "        \"f1_nov_neg\": 2 * ret_help[\"precision_nov_neg\"] * ret_help[\"recall_nov_neg\"] / max(1e-4, ret_help[\"precision_nov_neg\"] + ret_help[\"recall_nov_neg\"]),\n",
    "        \"f1_valid_novel\": 2 * ret_help[\"precision_valid_novel\"] * ret_help[\"recall_valid_novel\"] / max(1e-4, ret_help[\"precision_valid_novel\"] + ret_help[\"recall_valid_novel\"]),\n",
    "        \"f1_valid_nonnovel\": 2 * ret_help[\"precision_valid_nonnovel\"] * ret_help[\"recall_valid_nonnovel\"] / max(1e-4, ret_help[\"precision_valid_nonnovel\"] + ret_help[\"recall_valid_nonnovel\"]),\n",
    "        \"f1_nonvalid_novel\": 2 * ret_help[\"precision_nonvalid_novel\"] * ret_help[\"recall_nonvalid_novel\"] / max(1e-4, ret_help[\"precision_nonvalid_novel\"] + ret_help[\"recall_nonvalid_novel\"]),\n",
    "        \"f1_nonvalid_nonnovel\": 2 * ret_help[\"precision_nonvalid_nonnovel\"] * ret_help[\"recall_nonvalid_nonnovel\"] / max(1e-4, ret_help[\"precision_nonvalid_nonnovel\"] + ret_help[\"recall_nonvalid_nonnovel\"])\n",
    "    })\n",
    "\n",
    "    ret.update({\n",
    "        \"f1_val_macro\": (ret[\"f1_validity\"] + ret[\"f1_val_neg\"])/2,\n",
    "        \"f1_nov_macro\": (ret[\"f1_novelty\"] + ret[\"f1_nov_neg\"])/2,\n",
    "        \"f1_macro\": (ret[\"f1_valid_novel\"]+ret[\"f1_valid_nonnovel\"]+ret[\"f1_nonvalid_novel\"]+ret[\"f1_nonvalid_nonnovel\"])/4\n",
    "    })\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29473795",
   "metadata": {},
   "source": [
    "### Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c81db8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_validity': 0.773972602739726,\n",
       " 'f1_novelty': 0.02247191011235955,\n",
       " 'f1_val_neg': 0.4107142857142857,\n",
       " 'f1_nov_neg': 0.7238095238095238,\n",
       " 'f1_valid_novel': 0.0,\n",
       " 'f1_valid_nonnovel': 0.6048387096774194,\n",
       " 'f1_nonvalid_novel': 0.0,\n",
       " 'f1_nonvalid_nonnovel': 0.417910447761194,\n",
       " 'f1_val_macro': 0.5923434442270059,\n",
       " 'f1_nov_macro': 0.37314071696094164,\n",
       " 'f1_macro': 0.25568728935965335}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_nov_metric(pred_val, np.array(data_dev[\"Validity\"]), pred_nov, np.array(data_dev[\"Novelty\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1678718",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0b88134-0fbc-4b4a-92b5-2662eedb3dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_validity': 0.6006600660066006,\n",
       " 'f1_novelty': 0.08264462809917357,\n",
       " 'f1_val_neg': 0.4423963133640553,\n",
       " 'f1_nov_neg': 0.7218045112781954,\n",
       " 'f1_valid_novel': 0.06896551724137931,\n",
       " 'f1_valid_nonnovel': 0.4598698481561822,\n",
       " 'f1_nonvalid_novel': 0.020618556701030924,\n",
       " 'f1_nonvalid_nonnovel': 0.27299703264094954,\n",
       " 'f1_val_macro': 0.521528189685328,\n",
       " 'f1_nov_macro': 0.40222456968868453,\n",
       " 'f1_macro': 0.2056127386848855}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_nov_metric(pred_val_test, np.array(data_test[\"Validity\"]), pred_nov_test, np.array(data_test[\"Novelty\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd154df",
   "metadata": {},
   "source": [
    "### Topic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "132d14a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Two-party system', 'Trying terrorist suspects in civilian courts', 'UN Security Council veto', 'US health care reform', 'Torture', 'US-Indian nuclear deal', 'Underground nuclear waste storage', 'US electoral college', 'US debt ceiling deal', 'Twin Towers reconstruction', 'TV viewing is harmful to children', 'U.S. Withdrawal From the United Nations', 'Unilateral US military strike inside Pakistan', 'Two-state solution to Israeli-Palestinian conflict', 'Using sanctions to end child labor', 'Turkey EU membership', 'Trying 9/11 terror suspects in NYC courts', 'US and NATO intervention in Libya', 'United Nations Standing Army', 'Trade vs aid', 'US offshore oil drilling', 'United Nations No Growth Budgets'}\n"
     ]
    }
   ],
   "source": [
    "print(set(data_train[\"topic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1923722-d420-4a3c-82a4-8dbbcb80c5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'War on Drugs', 'Video surveillance', 'Zoos', 'Vegetarianism', 'Wind energy', 'Wave power', 'Yucca Mountain nuclear waste repository', 'Warrantless wiretapping in the United States'}\n"
     ]
    }
   ],
   "source": [
    "print(set(data_dev[\"topic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "608f4b1a-fe91-4e21-9330-bdaf56f7c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Withdrawing from Iraq', 'War on Drugs', 'Zoos', 'Warrantless wiretapping in the United States', 'Year-round school', 'Zero tolerance law', 'Was the War in Iraq worth it?', 'Veal', 'Vegetarianism', 'Wind energy', 'Water privatization', 'Video games', 'Wave power', 'Yucca Mountain nuclear waste repository', 'Video surveillance'}\n"
     ]
    }
   ],
   "source": [
    "print(set(data_test[\"topic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc3ec0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Withdrawing from Iraq', 'Year-round school', 'Zero tolerance law', 'Was the War in Iraq worth it?', 'Veal', 'Water privatization', 'Video games'}\n"
     ]
    }
   ],
   "source": [
    "print(set(data_dev[\"topic\"]).symmetric_difference(set(data_test[\"topic\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "727311db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>Premise</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Validity</th>\n",
       "      <th>Validity-Confidence</th>\n",
       "      <th>Novelty</th>\n",
       "      <th>Novelty-Confidence</th>\n",
       "      <th>examples</th>\n",
       "      <th>examples_topic</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>stemmed_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [topic, Premise, Conclusion, Validity, Validity-Confidence, Novelty, Novelty-Confidence, examples, examples_topic, stemmed, stemmed_topic]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dev.loc[data_dev['topic'] == \"Two-party system\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cf9736c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trying 9/11 terror suspects in NYC courts             90\n",
       "Trying terrorist suspects in civilian courts          85\n",
       "US health care reform                                 80\n",
       "US offshore oil drilling                              70\n",
       "US and NATO intervention in Libya                     45\n",
       "Torture                                               45\n",
       "US-Indian nuclear deal                                35\n",
       "Two-state solution to Israeli-Palestinian conflict    35\n",
       "United Nations Standing Army                          30\n",
       "Turkey EU membership                                  30\n",
       "Unilateral US military strike inside Pakistan         25\n",
       "UN Security Council veto                              25\n",
       "Using sanctions to end child labor                    20\n",
       "US debt ceiling deal                                  20\n",
       "Twin Towers reconstruction                            20\n",
       "US electoral college                                  15\n",
       "Two-party system                                      15\n",
       "Underground nuclear waste storage                     15\n",
       "United Nations No Growth Budgets                      15\n",
       "TV viewing is harmful to children                     15\n",
       "U.S. Withdrawal From the United Nations               10\n",
       "Trade vs aid                                          10\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"topic\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
